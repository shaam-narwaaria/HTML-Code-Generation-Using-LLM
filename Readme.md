# ğŸš€ HTML Code Generation using Falcon-7B LLM Model

This repository contains the implementation for generating HTML code using **Falcon-7B**, a large language model fine-tuned using the **PEFT (Parameter Efficient Fine-Tuning)** technique. The goal is to generate accurate and semantically meaningful HTML code based on user prompts or natural language instructions.

---

## ğŸ§  Overview

In this project, we leverage **Falcon-7B**, a cutting-edge LLM, for HTML code generation. The model is fine-tuned using the **PEFT** technique, enabling efficient memory and computation usage without compromising performance.

---

## âœ¨ Features

- ğŸ”® **Falcon-7B** based text generation for HTML
- âš™ï¸ **PEFT-based fine-tuning** for lightweight training and inference
- ğŸ“ Generates structured and semantically correct HTML from natural language prompts
- ğŸ“Š Evaluation using **BLEU Score** and **custom metrics**
- ğŸ§ª Ready-to-use in **Google Colab** for training and inference

---

## ğŸ“‚ Dataset

We use a subset of the Alpaca-style instruction dataset fine-tuned for HTML:

- ğŸ“ [`html_alpaca` Dataset on Hugging Face](https://huggingface.co/datasets/ttbui/html_alpaca)

---

## âœ… Prerequisites

Before running the project, ensure the following dependencies are installed:

- Python â‰¥ 3.8
- PyTorch
- `transformers` (Hugging Face)
- `peft`
- `accelerate`
- `wandb`
- Other dependencies listed in `requirements.txt`

---

## âš™ï¸ How to Run on Google Colab

### ğŸ”¹ Step 1: Upload Notebook
- Upload the `html-generation.ipynb` notebook to [Google Colab](https://colab.research.google.com/)

### ğŸ”¹ Step 2: Setup Authentication
- ğŸ”‘ **Hugging Face**: Paste your Hugging Face token to login.
- ğŸ”‘ **Weights & Biases (Wandb)**: Paste your API key for tracking experiments.

### ğŸ”¹ Step 3: Run Inference
- Scroll to the **Inference Model** cell.
- Paste your desired test case from the `/input` folder into the prompt box.

---

## ğŸ“ˆ Evaluation Metrics

- **BLEU Score**: Evaluates n-gram similarity between generated and reference HTML.
- **Custom Metrics**:
  - âœ… Tag correctness
  - âœ… Structural accuracy
  - âœ… Styling and scripting analysis
  - âœ… Semantic correctness
  - âœ… Overall quality score

---

## ğŸ“ Project Structure
html-falcon7b-htmlgen/
â”œâ”€â”€ input/
â”‚   â””â”€â”€ sample_prompt.txt
â”œâ”€â”€ output/
â”‚   â””â”€â”€ sample_output.html
â”œâ”€â”€ html-generation.ipynb
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

---

## ğŸ‘¨â€ğŸ’» Author

- [Shambhoolal Narwaria](https://github.com/mr-narwaria)

---

## ğŸŒŸ Acknowledgments

- Hugging Face ğŸ¤— for Transformers and Datasets
- Wandb for experiment tracking
- The creators of the `html_alpaca` dataset

---

## ğŸ‰ Enjoy the Model!

Feel free to fork, contribute, and share your feedback! If you like this project, give it a â­ on GitHub.
